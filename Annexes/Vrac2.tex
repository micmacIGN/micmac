\chapter{Vrac -Bis }

Suite du Vrac. Regroupe plut\^ot des \'el\'ements un peu 
\'eloign\'e du traitement d'image et de la photogramm\'etrie.

%-------------------------------------------------------------------
%-------------------------------------------------------------------
%-------------------------------------------------------------------

\section{Introduction-Supression des inconnues auxiliaires}

\label{UnkAux:Var}

On d\'ecrit ici une technique utilis\'ee pour introduire les
points terrains dans les \'equation photogram\'etriques. L'int\'er\^et
de cette technique est d'avoir des formules "naturelles"
faisant intervenir la projection des points terrains dans
les images sans alourdir le calcul; en effet , une fois connues
toutes les \'equations impliquant le point, on le fait 
dispara\^itre par un jeu de subistitution (on n'augmente
donc pas la taille du syst\`eme quadratique final).

On consid\`ere une s\'erie de $K$ observations $\Ok$ faisant
intervenir deux jeux d'inconnues $X_i, i \in [1,N[$ et $Y_j, j \in [0,M[$:

\begin{equation}
   \Ok = \sum_{i=0}^N a^k_i X_i + \sum_{j=0}^M b^k_j Y_j + c_k = 0
\end{equation}

Ces observations font partie d'un plus grand syst\`eme \`a r\'esoudre
par moindres carr\'es.

Typiquement dans notre cas :

\begin{itemize}
    \item   les $X_i$ seront les trois inconnues de coordon\'ees
            d'un point terrain,   

     \item   les  $Y_j$ seront les inconnues de poses
             (orientation et centre optiques) et de calibration
	     (ie param\`etres extrins\`eques et intrins\`eques) des
	     images dans lesquelles se projette ce point;

     \item les $K$ observations correspondent \`a la projection du 
           point terrain \`a un position $x_k,y_k$ donn\'ees 
	   dans $\frac{K}{2}$  images (apr\'es lin\'earisation);

\end{itemize}

On note $X$ le vecteur colone de coordonn\'ees $X_k$:
On suppose que les $K$ observations sont les seules faisant intervenir
les $X_i$; la partie de la fonctionnelle impliquant les $X_i$ est :

\begin{equation}
     \mathcal{L}(X) = \sum_{k=0}^K  \Ok ^2
\end{equation}

On d\'eveloppe :

\begin{equation}
     \mathcal{L}(X) = \sum_{k \in [1,K[}  
     ( \sum_{i=0}^N a^k_i X_i + \sum_{j=0}^M b^k_j Y_j + c_k )^2
\end{equation}

\begin{equation}
     \mathcal{L}(X) = \sum_{k=0}^K
     \{ ( \sum_{i=0}^N a^k_i X_i)^2  
    +  2(\sum_{i=0}^N a^k_i X_i)(\sum_{j=0}^M b^k_j Y_j + c_k)
    +  (\sum_{j=0}^M b^k_j Y_j + c_k )^2 \}
\end{equation}


Le troisi\`eme terme  ne fait pas intervenir les $X_k$, il est trait\'e
comme une contribution "normale" qui va se rajouter \`a l'accumulateur
global; on n'en tient plus compte maintenant (on note $\mathcal{L}'(X)$
le terme ainsi simplifi\'e)



On note $\Lambda =(\lambda_{i_1,i_2})$ la matrice $N*N$ d\'efinie par :

\begin{equation}
    \lambda_{i_1,i_2} = \sum_{k=0}^K a^k_{i_1} a^k_{i_2}
\end{equation}


On note $\Gamma =\gamma_{i}$ le vecteur colonne  d\'efini par :


\begin{equation}
    \gamma_{i} = \sum_{k=0} a^k_i (\sum_{j=N}^M b^k_j Y_j + c_k)
\end{equation}

On peut alors \'ecrire :

\begin{equation}
     \mathcal{L}'(X) =  ^t X \Lambda X + 2 ^t \Gamma  X
\end{equation}


Raisonnement "avec les mains": pour minimiser la fonctionnelle
globale on a tout int\'er\^et \`a ce que $X$ prenne la valeur
mininum en fonction de $Y$.

Classiquement, s'agissant d'une fonctionnelle quadratique,
le miminum de $\mathcal{L}'$ est atteint en $X= -\Lambda^{-1} \Gamma $ et
la valeur de ce minimum est  :

\begin{equation}
     \mathcal{L}'^0 =   - ^t \Gamma  \Lambda^{-1} \Gamma 
\end{equation}

Il suffit donc de rajouter le terme $\mathcal{L}'^0$ qui est un terme
quadratique en $Y$. 
Explicitons les valeur de ce terme, on pose :


\begin{equation}
     A_i = \sum_{k=0} a^k_i  c_k ,
     B_{i,j} = \sum_{k=0} a^k_i  b^k_j
\end{equation}

Et on a :

\begin{equation}
  \Gamma = 
 \begin{pmatrix}  \gamma_0 \\ \dots   \\    \gamma_{N-1} \end{pmatrix}
 = \begin{pmatrix}  A_0 \\ \dots   \\  A_{N-1}  \end{pmatrix}
   +  \begin{pmatrix}  B_{0,0} &  B_{0,1} & ... & B_{0,M-1} \\ 
                      \dots  & \dots  & \dots & \dots   \\    
                       B_{N-1,0} &  B_{N-1,1} & ... & B_{N-1,M-1} 
      \end{pmatrix}
      \begin{pmatrix}  Y_0 \\  Y_1 \\ \dots    \\ Y_{M-1}\end{pmatrix}
\end{equation}


Soit, en notant $A$ le vecteur colonne de coordonn\'ees $A_i$ et $B$ la matrice
dont les \'el\'ements sont $B_{i,j}$ :

\begin{equation}
  \Gamma =   A + B Y
\end{equation}

Et  :
\begin{equation}
     \mathcal{L}'^0 =   -  ^t Y (^t B \Lambda^{-1} B ) Y -2 (^tA \Lambda^{-1} B) Y - ^tA \Lambda^{-1} A
\end{equation}


Il n'y a plus qu'a rajouter ce terme quadratique \`a l'accumulateur.

%-------------------------------------------------------------------
%-------------------------------------------------------------------
%-------------------------------------------------------------------

\section{Formulation alg\'ebrique du traitement des inconnues auxiliaires}

\label{UnkAux:Algeb}

On donne une formulation purement alg\'ebrique de la technique pr\'ec\'edente.
Cette formulation bien que moins intuitive est sans doute plus facile
d'emploi.

On a un syst\`eme lin\'eaire \`a r\'esoudre dans lesquelle on classe
les inconnues en trois cat\'egories :

\begin{itemize}
   \item    $X=(X_i)$ les inconnues \`a \'eliminer;
   \item    $Y=(X_j)$ les inconnues interf\'erant avec les $X_i$;
   \item    $Z=(Z_k)$ les inconnues n'interf\'erant pas avec les $X_i$;
\end{itemize}

Avec la notation matrice bloc on \'ecrit :

\begin{equation}
 \begin{pmatrix}  \Lambda &  B & 0 \\ 
                 B' &    M_{1,1} & M_{2,1}  \\    
	         0     &  M_{1,2} &  M_{2,2}
  \end{pmatrix}
   \begin{pmatrix} X \\ Y \\ Z \end{pmatrix}
   = \begin{pmatrix} A \\ C_1 \\  C_2 \end{pmatrix}
\end{equation}

Ce que l'on peut \'ecrire :

\begin{equation}
 \begin{cases}  
          \Lambda X + B Y & = A   \\
	    B'X +  M_{1,1}Y +  M_{2,1} Z & = C_1 \\
	           M_{1,2}Y +  M_{2,2} Z & = C_2 \\
  \end{cases}
\end{equation}

On a :
\begin{equation}
    X = \Lambda^{-1} (A-BY)
\end{equation}

On substitue  dans la deuxi\`eme \'equation:

\begin{equation}
        -B' \Lambda^{-1} BY  +  M_{1,1}Y +  M_{2,1} Z  = C_1 -B'\Lambda^{-1} A
\end{equation}

On a donc le syst\`eme suivant d'o\`u $X$ est \'elimini\'e :

\begin{equation}
 \begin{pmatrix}  
                   M_{1,1} -B' \Lambda^{-1} B & M_{2,1}  \\    
	           M_{1,2} &  M_{2,2}
  \end{pmatrix}
   \begin{pmatrix}  Y \\ Z \end{pmatrix}
   = \begin{pmatrix}  C_1 -B'\Lambda^{-1} A \\  C_2 \end{pmatrix}
\end{equation}

%-------------------------------------------------------------------
%-------------------------------------------------------------------
%-------------------------------------------------------------------

\section{Précision et corrélation sur les paramères}

Code voir Prec-Cor-Param.txt



