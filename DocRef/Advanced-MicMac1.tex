\chapter{Advanced matching, theoretical aspect}


This chapter presents some theoretical point and mathematical notation
useful for a more detailed presentation of MicMac's parameters.


\section{Generalities}

\subsection{Geometric notations}

Generally, we have $N$ images to match, they are noted $Im_k(u,v)_{k\in [1,N]}$.
We use the notations:


\begin{itemize}
   \item \ETer,  with  $\ETer = \RR ^2$,  the "ground" space, this is
         the space in which the depth is computed; note that according
         to the selected restitution geometry , this space can be the
         euclidean space, or the "master" image space;


   \item \EPx, with  $\EPx = \RR$ or   $\EPx = \RR ^2$ , the "disparity" space
          \footnote{according to restitution geometry , it can be "real" disparity, depth of euclidean $Z$};
          we note  \DimPx the dimension of disparity space;

   \item \EIm the space of  k\KTH image;
\end{itemize}


Internally, for MicMac, the geometry of images is manipulated using
a function  $\pi_k$ for each image:

\begin{equation}
   \pi_k :   \ETer \otimes \EPx \rightarrow \EIm
\end{equation}
\begin{equation}
              (x,y,p_x)  \stackrel{\pi_k}{\rightarrow}  (u,v)
\end{equation}

With $p_x=z$ when $\EPx = \RR$ and   $p_x=(p_{x_1},p_{x_2})$
when $\EPx = \RR^2$. For a given $p_x$, the function $\pi_k$,
considered as function from \ETer into  \EIm, are one to one mapping,
and we write $\pi_k^{-1}$ the inverse function defined by :


\begin{equation}
   \pi_k^{-1} :   \EIm \otimes \EPx \rightarrow \ETer
\end{equation}

\begin{equation}
   \pi_k^{-1}(\pi_k(x,y,p_x),p_x) = (x,y)
\end{equation}


The result of the matching process is a function \FPx from
\ETer to \EPx:

\begin{equation}
   \FPx :   \ETer  \rightarrow  \EPx
\end{equation}

We note ${\FPx}^d,d\in [1,\DimPx]$ the components of \FPx.

 % - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

\subsection{Notation for quantification}

\label{Disc:Quant}


In most cases, MicMac works by discretization of
spaces \ETer and \EPx (this is not the case for
variational approaches, still undeveloped in MicMac).
At each stage of muti resolution process (see ~\ref{Appr:Multi:Resol}),
steps of quantification must be defined: $\Delta^{xy}$ for \ETer ,
and $\Delta^{px}_k, k \in [1,\DimPx]$ for  \EPx.



For a given stage, a resolution  step \DeltaI of image space
is also selected ; in most cases  \DeltaI is chosen to be equivalent
to $\Delta^{xy}$ (but there can be exceptions).
For each used $\DeltaI$, MicMac computes images down sampled of a size $\DeltaI$, we note $Im_{ki}^{\DeltaI}$  these images
and $\EIm^{\DeltaI}$  their associated spaces.



For a given quantification, we define a discrete version
of $\pi_k$ by ;


\begin{equation}
   p_k : \ZZ^2 \otimes\ZZ^{\DimPx} \rightarrow \EIm^{\DeltaI}
\end{equation}

\begin{equation}
   p_k(i,j,u,v) = \frac{\pi_k(i*\Delta^{xy},j*\Delta^{xy},u*\Delta^{px}_1,v*\Delta^{px}_2)}{\DeltaI}
\label{EQ:Disc:Proj}
\end{equation}

For given steps $\Delta^{xy}$ and $\Delta^{px}_k$, the objective of
the matching process is to compute a table $F_{px}$, from $\ZZ^2$  to
$\ZZ^{\DimPx}$, which is a discrete version of \FPx:


\begin{equation}
   F_{px}(i,j)^k = \frac{\FPx(i*\Delta^{xy},j*\Delta^{xy})}{\Delta^{px}_k}
\end{equation}

    % - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -



    % - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
    % - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
    % - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

\section{Energetic formulation and regularization}

\subsection{Generalities}

\label{AM:Appr:Energ}


The objective of the matching process is to compute a function
\FPx , respecting some \emph{a priori} constraints \footnote{regularity}
and such that the $Im_k(\pi_k(x,y,\FPx(x,y))),k\in [1,N]$ are similar.



Let's write :

\begin{itemize}
     \item $\Ress(x,y,p_x) \geq 0$  a criteria measuring the
            local similarity between
            $Im_k(\pi_k(x,y),p_x)_{k\in [1,N]}$, with $\Ress=0$
            when image are perfectly identical ($\Ress$ is the image term);

      \item $\nabla(\FPx)$ the gradient of \FPx,
             $\nabla(U) = (\frac{U}{\partial x}, \frac{U}{\partial y})$

      \item $\| \nabla(\FPx) \|^{reg}$ a norm on the gradient, which is used as
            a regularity criteria (it penalizes the variation of \FPx);
\end{itemize}


In energetic formulation, one tries to minimize a global energetic function $\Energ(\FPx)$  :

\begin{equation}
   \Energ(\FPx) = \iint_\ETer \Ress(x,y,\FPx(x,y)) + \| \nabla(\FPx) \|^{reg}
\end{equation}


By default, the cost function used by MicMac are $L_1$  :

\begin{equation}
\label{CostGrad:Lun:DDeux}
   \| \nabla(\FPx) \|^{reg} = \alpha_1*|\nabla(\FPx^1)|+\alpha_2*|\nabla(\FPx^2)|
\end{equation}


Note that in this equation $\alpha_1$ is the regularization on first
component of disparity and $\alpha_2$ the regularization  on the
second component (usable only when $\DimPx=2$). Typically we will have :

\begin{itemize}
   \item $\alpha_1 \approx \alpha_2$ for geometry "really" bi-dimensional
         (for example when matching is used for isotropic deformation  measurement);

   \item $\alpha_1 \ll \alpha_2$ for geometry where  $\FPx^2$ is used to correct
         the default of geometric models ( $\FPx^2$ modelize the transverse disparity);

\end{itemize}



MicMac offers (too ?) many option for optimization:

\begin{itemize}
    \item  options on the cost function;
    \item  options on the  disparity dimension;
    \item  options  on the choice of the algorithm that will be used
           for optimization of the function  $\Energ$;
\end{itemize}

However there are several restrictions in the combination of these options.
The following table sums up the main restrictions between the choice of algorithm
and options:


\begin{tabular} { c | c | c | c| c| c | c} % \\  \hline

 {\bf Type }        &  {Quantification} & {Speed} & {Cost func} &  {Disp Dim} &  {D Opt} & {Optim}\\  \hline \hline
 {\bf Prog Dyn }    &  {Oui} & {+++} &          { Any} & {1,2} & {1} &{approximate}\\  \hline
 {\bf Cox-Roy }     &  {Oui} & {++} &           {$L_1$} & {1} & {1,2} & {exact minimum}\\  \hline
 {\bf Differential }&  {Non} & {+} &            {$L_2$} & {1,2} & {1,2} &{local descent} \\  \hline  \hline
 {\it Dequant }     &  {?} & {++++} &           { ?} & {1,2} & {1,2} &{ post filtering}\\  \hline
 {\it MaxOfScore }  &  {Oui} & {++++} &         { ?} & {1,2} & {1,2} &{  ?} \\  \hline

\end{tabular}


Let's comment some columns:

\begin{itemize}
  \item {\bf Cost func} describes which cost functions are available for each algorithm;
        this refers to MicMac implementation and not theoretical possibility (for example
        max flow allows any convex function);

  \item {\bf Disp  Dim}  indicates that the algorithm can handle 2 dimensional disparity;

  \item {\bf Disp  Opt}  indicates that the algorithm makes the optimization in two dimensions;

\end{itemize}


Let's comment some line :

\begin{itemize}
   \item   {\bf   Cox-Roy } Cox-Roy  is the Cox and Roy implementation of Max Flow;
   \item   {\bf   Differential } are no longer available now (but they may be added again).
\end{itemize}

    % - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
    % - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
    % - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

\COM{
\subsection{Dynamic programming}

La programmation dynamique se limite au cas o\`u l'espace de d\'epart est
ordonn\'e (donc de dimension $1$) et l'espace d'arriv\'ee est quantifi\'e.
Ces limitations prises en compte, c'est une
une m\'ethode de port\'ee assez g\'en\'erale
pour optimiser   des application de  $\ZZ$ dans $\ZZ^k$.

En st\'er\'eo vision (et en traitement d'image de mani\`ere plus
g\'en\'erale)  o\`u l'espace de d\'epart est de dimension $2$,
on est souvent conduit \`a balayer l'image suivant les
lignes (ou les colonnes, ou n'importe quelle direction)
pour utiliser la programmation dynamique.
Pour chaque balayage on a alors une application de $\ZZ$ dans $\ZZ^k$.


Formellement, on peut d\'ecrire ainsi le fonctionnement :

\begin{itemize}
   \item on dispose de $N$ positions $P_k ,  k \in [1,N]$;

   \item pour chaque position, on dispose de  l'ensemble des
         \'etats possibles que peut prendre cette position
         $E_k ={e^k_1,\dots,e^k_{n_k}} $

   \item une solution $S$ est une suite correspondant \`a la
          s\'election de l'\'etat de chaque position $\{e^1_{S(1}) \dots e^N_{S(N)}\}$;

    \item  chaque \'etat poss\`ede un cout intrins\`eque $C^I(e^k_i)$ et chaque
           couple d'\'etats successifs poss\`ede un co\^ut de transition $C^T(e^k_i,e^{k+1}_j)$
\end{itemize}

Le co\^ut global d'une solution est d\'efini par :

\begin{equation}
  \mathcal C (S) = \sum_{k=1}^N  C^I(e^k_{S(k)}) + C^T(e^k_{S(k)},e^{k+1}_{S(k+1)})
\end{equation}

Une fois effectu\'e l'op\'eration de balayage, la transcription pour chaque
ligne au probl\`eme d'appariement  est imm\'ediate :

\begin{itemize}
    \item les positions sont les pixels;

    \item les \'etats d'un pixels sont les paralaxes qu'il peut prendre
          en fonction du contexte (nappes englobantes \dots);

    \item une solution est un champs de paralaxe;

    \item les co\^uts intrins\`eques portent  l'attache
          aux donn\'ees (fonction de la corr\'elation obtenue pour chaque pixel
          lorsqu'on le reprojette \`a la parallaxe $e^k_j$);

    \item les co\^uts  de transitions  portent  l'a priori;
          par exemple, si la paralaxe correspondant \`a
          l'\'etat $e^k_i$  s'\'ecrit $({Px^1}^k_i,{Px^2}^k_i)$, on pourrait
          utiliser l'\'equation~\ref{CostPrgDyn:DDeux} comme sch\'ema de
          discr\'etisation de~\ref{CostGrad:Lun:DDeux};

    \item on pourrait  aussi  mod\'eliser des co\^uts quadratiques avec
          l'\'equation~\ref{CostPrgDyn:Quad};

    \item on peut imposer diff\'erentes contraintes, par exemple
           que les solution retenue respectent un crit\`ere
          de pente maximum $ \mathcal P_{max}$  en rajoutant une r\`egle du type
          de l'\'equation~\ref{CostPrgDyn:Pmax};


\end{itemize}


\begin{equation}
\label{CostPrgDyn:DDeux}
C^T(e^k_i,e^{k+1}_j) =    \alpha_1* |{Px^1}^k_i-{Px^1}^{k+1}_j|
                          + \alpha_2* |{Px^2}^k_i-{Px^2}^{k+1}_j|
\end{equation}

\begin{equation}
\label{CostPrgDyn:Quad}
C^T(e^k_i,e^{k+1}_j) =    \alpha_1* ({Px^1}^k_i-{Px^1}^{k+1}_j)^2
\end{equation}

\begin{equation}
\label{CostPrgDyn:Pmax}
   |{Px^1}^k_i-{Px^1}^{k+1}_j| > \mathcal P_{max} \Rightarrow  C^T(e^k_i,e^{k+1}_j) = + \infty
\end{equation}


La programmation dynamique fournit alors un algorithme rapide pour
calculer la solution $\mathcal S_{min} $ qui minimise le
co\^ut $ \mathcal C (S) $.
Rappelons le principe :

\begin{itemize}

\label{Def:CPlusMin}
   \item on cherche \`a calculer la fonction $ \mathcal C^+_{min} (e^k_i)$
   qui correspondent au co\^ut de la plus petite sous suite de
   taille $k$ se  terminant par $k$ (c.a.d.  les suites
   de la forme $e^0_{i_0} \dots e^{k-1}_{i_{k-1}} e^k_i$);

   \item on effectue le calcul en parcourant les positions
         suivant les indice croissants;

   \item  en rajoutant un sommet neutre d'indice $0$ en d\'ebut on a
          initialement  $ \mathcal C^+_{min} (e^0_0) = 0$

   \item on utilise ensuite la r\'ecurence donn\'ee par
         l'\'equation~\ref{CostPrgDyn:Recur}, en memorisant
         pour chaque sommet celui ayant conduit au calcul
         du minimum ( son "p\`ere");

\end{itemize}

\begin{equation}
\label{CostPrgDyn:Recur}
 \mathcal C^+_{min} (e^{k+1}_i) =  C^I(e^{k+1}_i)+Min_{j\in[1,n_k]}(
             \mathcal  C^+_{min} (e^k_j) +  C^T(e^k_j,e^{k+1}_i)
 )
\end{equation}

Arriv\'e  \`a la position $N$, l'\'etat qui minimise $\mathcal  C^+_{min} (e^N_j) $
est celui par lequel se termine  la solution $\mathcal S_{min} $, il
suffit alors de parcourir les position vers l'arri\`ere \`a partir de cet
\'etat en remontant aux "p\`eres" pour obtenir l'ensemble de  la
s\'erie $\mathcal S_{min} $.


    % - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
    % - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
    % - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

\section{Programmation dynamique "multi directionnelle"}

\label{ProgDyn:MulDir}

Le balayage directionnel, n\'ecessaire pour appliquer la
programmation dynamique \`a des images, conduit \`a des
artefacts qui peuvent \^etre assez g\'enants.
MicMac propose un principe de programmation dynamique "multi-directionnel"
qui permet de limiter ces effets.
Pour chaque direction de balayage, on
calcul e:

\begin{itemize}
   \item d'une part $ \mathcal C^+_{min} (e^k_i)$ d\'ecrit
    en~\ref{Def:CPlusMin};

   \item d'autre part $ \mathcal C^-_{min} (e^k_i)$ correspondant
   au co\^ut de la plus petite sous suite de
   taille $N-k$  commen\c{c}ant par $k$  et calcul\'ee selon
   le m\^eme principe  (mais par un parcours arri\`ere);

\end{itemize}

On pose :

\begin{equation}
  \mathcal C_{min}(e^k_i) = \mathcal C^+_{min} (e^k_i) + \mathcal C^-_{min} (e^k_i) - C^I(e^k_i)
\end{equation}

On montre facilement que $C_{min}(e^k_i)$ est le co\^ut de la plus
petite solution contrainte \`a passer par $e^k_i$.
On pose finalement :

\begin{equation}
    \Delta_{min}(e^k_i) = \mathcal C_{min}(e^k_i) -  \mathcal C ( \mathcal S_{min})
\end{equation}

La valeur $\Delta_{min}(e^k_i)$ s'interpr\`ete facilement comme
le surco\^ut, par rapport \`a la solution optimale qu'il y a \`a passer
par l'\'etat $e^k_i$. On dispose d'une information sensiblement
plus riche que la programmation dynamique  habituelle qui ne fournit
que la solution optimale : on dispose maintenant, pour chaque \'etat,
d'une mesure \`a valeur r\'eelle quantifiant  quelle distance, au sens
du crit\`ere \`a minimiser, le s\'epare de la solution optimum.


L'int\'er\^et est d'avoir une mesure qui va permettre d'aggr\'eger les
r\'esultat obtenus par des balayages effectu\'es dans plusieurs
directions de l'images afin de limiter les effets directionnels.
On poura par exemple balayer l'image selon $D$ directions
correspondant \`a des angles $\frac{d\pi}{D}$ et aggr\'eger
suivant une des m\'ethodes ci-dessous chaque $\Delta^d_{min}(e^k_i)$ obtenu~:

\begin{enumerate}
   \item calculer la moyenne des $\Delta^d_{min}(e^k_i)$;
   \item calculer le max des $\Delta^d_{min}(e^k_i)$;
   \item utiliser chaque $\Delta^d_{min}(e^k_i)$ comme co\^ut d'entr\'ee
         pour le prochain balyage (m\'ethode dite r\'eentrante);
\end{enumerate}




    % - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
    % - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
    % - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

\section{Algorithmes de flots}

Lorsque la parallaxe est de dimension $1$,
les algorithmes de "flots maximum / coupe minimum" permettent
de trouver le minimum exact du cit\`ere \'energ\'etique
en $L_1$. Une fois le prob\`eme discr\'etis\'e et quantifi\'e,
la fonctionnelle s'\'ecrit :


\begin{equation}
  \label{CostGrad:MaxFlot}
  E(Z) = \Sigma _{x,y}  (A(x,y,Z(x,y)) + \alpha *  \Sigma _{u,v \in V}P^{ds}_{u,v}|Z(x,y)-Z(x+u,y+v)|)
\end{equation}

Avec $Z$ la fonction inconnue, $A$ le crit\`ere d'attache aux donn\'ees,
$\alpha$ le coefficient pond\'erant l'\emph{a priori}, $V$ le voisinage
utilis\'ee (typiquement $4$ ou $8$ voisinage), et $P^{ds}_{u,v}$ une
\'eventuelle pond\'eration du voisinage.
L'impl\'ementation propos\'ee par MicMac est bas\'ee sur le code
de Cox \& Roy d\'ecrite dans~\cite{CoxRoy}.




    % - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
    % - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
    % - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

\section{Algorithmes de d\'equantification}

\label{DUMG:DEQUANTIF}

    % - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
    % - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
    % - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -


\section{Algorithmes variationnels}

Pas encore implant\'es.


}

